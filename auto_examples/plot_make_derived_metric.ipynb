{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Making Derived Metrics\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the use of the\r\n`fairlearn.metrics.make_derived_metric`{.interpreted-text role=\"func\"}\r\nfunction. Many higher-order machine learning algorithms (such as\r\nhyperparameter tuners) make use of scalar metrics when deciding how to\r\nproceed. While the `fairlearn.metrics.MetricFrame`{.interpreted-text\r\nrole=\"class\"} has the ability to produce such scalars through its\r\naggregation functions, its API does not conform to that usually expected\r\nby these algorithms. The\r\n`~fairlearn.metrics.make_derived_metric`{.interpreted-text role=\"func\"}\r\nfunction exists to bridge this gap.\r\n\r\n# Getting the Data\r\n\r\n*This section may be skipped. It simply creates a dataset for\r\nillustrative purposes*\r\n\r\nWe will use the well-known UCI \\'Adult\\' dataset as the basis of this\r\ndemonstration. This is not for a lending scenario, but we will regard it\r\nas one for the purposes of this example. We will use the existing\r\n\\'race\\' and \\'sex\\' columns (trimming the former to three unique\r\nvalues), and manufacture credit score bands and loan sizes from other\r\ncolumns. We start with some uncontroversial [import]{.title-ref}\r\nstatements:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import functools\nimport numpy as np\n\nimport sklearn.metrics as skm\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.pipeline import Pipeline\nfrom fairlearn.metrics import MetricFrame, make_derived_metric\nfrom fairlearn.metrics import accuracy_score_group_min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we import the data, dropping any rows which are missing data:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = fetch_openml(data_id=1590, as_frame=True)\nX_raw = data.data\ny = (data.target == \">50K\") * 1\nA = X_raw[[\"race\", \"sex\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now going to preprocess the data. Before applying any transforms,\r\nwe first split the data into train and test sets. All the transforms we\r\napply will be trained on the training set, and then applied to the test\r\nset. This ensures that data doesn\\'t leak between the two sets (this is\r\na serious but subtle [problem in machine\r\nlearning](https://en.wikipedia.org/wiki/Leakage_(machine_learning))).\r\nSo, first we split the data:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(\n    X_raw, y, A, test_size=0.3, random_state=12345, stratify=y\n)\n\n# Ensure indices are aligned between X, y and A,\n# after all the slicing and splitting of DataFrames\n# and Series\n\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\nA_train = A_train.reset_index(drop=True)\nA_test = A_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we build two `~sklearn.pipeline.Pipeline`{.interpreted-text\r\nrole=\"class\"} objects to process the columns, one for numeric data, and\r\nthe other for categorical data. Both impute missing values; the\r\ndifference is whether the data are scaled (numeric columns) or one-hot\r\nencoded (categorical columns). Imputation of missing values should\r\ngenerally be done with care, since it could potentially introduce\r\nbiases. Of course, removing rows with missing data could also cause\r\ntrouble, if particular subgroups have poorer data quality.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "numeric_transformer = Pipeline(\n    steps=[\n        (\"impute\", SimpleImputer()),\n        (\"scaler\", StandardScaler()),\n    ]\n)\ncategorical_transformer = Pipeline(\n    [\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n    ]\n)\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our preprocessor defined, we can now build a new pipeline which\r\nincludes an Estimator:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unmitigated_predictor = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\n            \"classifier\",\n            LogisticRegression(solver=\"liblinear\", fit_intercept=True),\n        ),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the pipeline fully defined, we can first train it with the training\r\ndata, and then generate predictions from the test data.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unmitigated_predictor.fit(X_train, y_train)\ny_pred = unmitigated_predictor.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating a derived metric\r\n\r\nSuppose our key metric is the accuracy score, and we are most interested\r\nin ensuring that it exceeds some threshold for all subgroups We might\r\nuse the `~fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}\r\nas follows:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "acc_frame = MetricFrame(\n    metrics=skm.accuracy_score,\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=A_test[\"sex\"]\n)\nprint(\"Minimum accuracy_score: \", acc_frame.group_min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can create a function to perform this in a single call using\r\n`~fairlearn.metrics.make_derived_metric`{.interpreted-text role=\"func\"}.\r\nThis takes the following arguments (which must always be supplied as\r\nkeyword arguments):\r\n\r\n-   `metric=`, the base metric function\r\n-   `transform=`, the name of the aggregation transformation to perform.\r\n    For this demonstration, we want this to be `'group_min'`\r\n-   `sample_param_names=`, a list of parameter names which should be\r\n    treated as sample parameters. This is optional, and defaults to\r\n    `['sample_weight']` which is appropriate for many metrics in\r\n    [scikit-learn]{.title-ref}.\r\n\r\nThe result is a new function with the same signature as the base metric,\r\nwhich accepts two extra arguments:\r\n\r\n> -   `sensitive_features=` to specify the sensitive features which\r\n>     define the subgroups\r\n> -   `method=` to adjust how the aggregation transformation operates.\r\n>     This corresponds to the same argument in\r\n>     `fairlearn.metrics.MetricFrame.difference`{.interpreted-text\r\n>     role=\"meth\"} and\r\n>     `fairlearn.metrics.MetricFrame.ratio`{.interpreted-text\r\n>     role=\"meth\"}\r\n\r\nFor the current case, we do not need the `method=` argument, since we\r\nare taking the minimum value.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "my_acc = make_derived_metric(metric=skm.accuracy_score, transform=\"group_min\")\nmy_acc_min = my_acc(y_test, y_pred, sensitive_features=A_test[\"sex\"])\nprint(\"Minimum accuracy_score: \", my_acc_min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To show that the returned function also works with sample weights:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_weights = np.random.rand(len(y_test))\n\nacc_frame_sw = MetricFrame(\n    metrics=skm.accuracy_score,\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=A_test[\"sex\"],\n    sample_params={\"sample_weight\": random_weights},\n)\n\nfrom_frame = acc_frame_sw.group_min()\nfrom_func = my_acc(\n    y_test,\n    y_pred,\n    sensitive_features=A_test[\"sex\"],\n    sample_weight=random_weights,\n)\n\nprint(\"From MetricFrame:\", from_frame)\nprint(\"From function   :\", from_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The returned function can also handle parameters which are not sample\r\nparameters. Consider `sklearn.metrics.fbeta_score`{.interpreted-text\r\nrole=\"func\"}, which has a required `beta=` argument (and suppose that\r\nthis time we are most interested in the maximum difference to the\r\noverall value). First we evaluate this with a\r\n`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fbeta_03 = functools.partial(skm.fbeta_score, beta=0.3)\nfbeta_03.__name__ = \"fbeta_score__beta_0.3\"\n\nbeta_frame = MetricFrame(\n    metrics=fbeta_03,\n    y_true=y_test,\n    y_pred=y_pred,\n    sensitive_features=A_test[\"sex\"],\n    sample_params={\"sample_weight\": random_weights},\n)\nbeta_from_frame = beta_frame.difference(method=\"to_overall\")\n\nprint(\"From frame:\", beta_from_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And next, we create a function to evaluate the same. Note that we do not\r\nneed to use `functools.partial`{.interpreted-text role=\"func\"} to bind\r\nthe `beta=` argument:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "beta_func = make_derived_metric(metric=skm.fbeta_score, transform=\"difference\")\n\nbeta_from_func = beta_func(\n    y_test,\n    y_pred,\n    sensitive_features=A_test[\"sex\"],\n    beta=0.3,\n    sample_weight=random_weights,\n    method=\"to_overall\",\n)\n\nprint(\"From function:\", beta_from_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pregenerated Metrics\r\n\r\nWe provide a number of pregenerated metrics, to cover common use cases.\r\nFor example, we provide a `accuracy_score_group_min()` function to find\r\nthe minimum over the accuracy scores:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from_myacc = my_acc(y_test, y_pred, sensitive_features=A_test[\"race\"])\n\nfrom_pregen = accuracy_score_group_min(\n    y_test, y_pred, sensitive_features=A_test[\"race\"]\n)\n\nprint(\"From my function :\", from_myacc)\nprint(\"From pregenerated:\", from_pregen)\nassert from_myacc == from_pregen"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}